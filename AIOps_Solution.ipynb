{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **How to Run (Notebook Version)**\n",
        "\n",
        "1. ***Clone repo / upload files to Colab or Vertex AI Workbench***\n",
        "\n",
        "2. **Make sure you have:**\n",
        "\n",
        "*  A GCP project\n",
        "*  Vertex AI API enabled\n",
        "\n",
        "\n",
        "*   Gemini 2.5 Flash available in your region"
      ],
      "metadata": {
        "id": "PCl2a9HLlyjW"
      },
      "id": "PCl2a9HLlyjW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Install dependencies in the notebook**"
      ],
      "metadata": {
        "id": "qIIjJP9Om3uS"
      },
      "id": "qIIjJP9Om3uS"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR2zUm8O4UqL",
        "outputId": "dfd07413-3931-4072-e405-de20f073126b"
      },
      "id": "XR2zUm8O4UqL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.12/dist-packages (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from gcsfs) (3.13.1)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.12/dist-packages (from gcsfs) (2025.3.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (from gcsfs) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from gcsfs) (2.32.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.22.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (2.28.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->gcsfs) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.71.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (6.33.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "\n",
        "import gcsfs\n",
        "import json\n",
        "from docx import Document\n",
        "import os\n",
        "\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "path = \"gs://demo-incidents/Incident.docx\"\n",
        "\n",
        "incidents = []\n",
        "\n",
        "# Create a temporary local file to write the GCS content\n",
        "local_file_path = \"temp_incident.docx\"\n",
        "with fs.open(path, \"rb\") as gcs_file:\n",
        "    with open(local_file_path, \"wb\") as local_file:\n",
        "        local_file.write(gcs_file.read())\n",
        "\n",
        "document = Document(local_file_path)\n",
        "full_text = []\n",
        "for para in document.paragraphs:\n",
        "    if para.text.strip(): # Only add non-empty paragraphs\n",
        "        full_text.append(para.text.strip())\n",
        "\n",
        "# For now, let's just store the full text as a single \"incident\"\n",
        "# The user might need to clarify how incidents are structured within the DOCX.\n",
        "if full_text:\n",
        "    incidents.append({\"content\": \"\\n\".join(full_text)})\n",
        "\n",
        "# Clean up the temporary local file\n",
        "os.remove(local_file_path)\n",
        "\n",
        "len(incidents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7EiqWc-ALYA",
        "outputId": "2e4ad953-d44c-4356-f3df-a37d02c4a90d"
      },
      "id": "X7EiqWc-ALYA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Initialize Vertex AI**"
      ],
      "metadata": {
        "id": "G84h1IQ4nOiu"
      },
      "id": "G84h1IQ4nOiu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Send each incident to Gemini 1.5\n",
        "\n",
        "!pip install google-cloud-aiplatform --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78SIPT2uCfBH",
        "outputId": "91e985e5-f74e-444d-d535-f4119b908e8f"
      },
      "id": "78SIPT2uCfBH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.127.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.28.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (6.33.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.46.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.11.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely<3.0.0->google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Run the notebook cells in order:**\n",
        "\n",
        "*   Load incidents\n",
        "*   Call Gemini to enrich and summarize\n",
        "\n",
        "*   Cluster incidents\n",
        "*   Generate RCA per cluster\n",
        "\n",
        "*   Export final_aiops_report.json\n",
        "\n",
        "**6. After execution, download final_aiops_report.json from the notebook file browser.**"
      ],
      "metadata": {
        "id": "ArwsyI46nzEx"
      },
      "id": "ArwsyI46nzEx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Gemini + initialize Vertex AI\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(\n",
        "    project = \"my-sentiment-478307\",\n",
        "    location = \"us-central1\"\n",
        ")\n",
        "\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmEnbtpgKFEE",
        "outputId": "dc2df32e-ff53-485f-916b-b31d03588767"
      },
      "id": "FmEnbtpgKFEE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n",
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the function to send one incident to Gemini\n",
        "\n",
        "def analyze_incident(incident):\n",
        "  prompt = f\"\"\" You are an AI assistant that analyzes IT Incidents.\n",
        "\n",
        "  INCIDENT:\n",
        "  {incident}\n",
        "\n",
        "  Task:\n",
        "  1. Classify severity: Critical / High / Medium / Low\n",
        "  2. Identify root-cause category (Server, Network, DB, Application, Infra, Cloud)\n",
        "  3. Suggest best next action\n",
        "  4. Is this correlated to any known common issue? (Y/N)\n",
        "  5. Provide correlation signature (short text)\n",
        "\n",
        "  Return JSON Only:\n",
        "  {{\n",
        "    \"severity\": \"\",\n",
        "    \"category\": \"\",\n",
        "    \"next_action\": \"\",\n",
        "    \"correlation\": \"\",\n",
        "    \"signature\": \"\"\n",
        "  }}\n",
        "  \"\"\"\n",
        "  response = model.generate_content(prompt)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "1IQbeHbeM3FP"
      },
      "id": "1IQbeHbeM3FP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send ALL incident to Gemini\n",
        "\n",
        "all_incident_results = []\n",
        "for i, incident_data in enumerate(incidents):\n",
        "  print(f\"Processing incident {i+1}/{len(incidents)}\")\n",
        "  analysis = analyze_incident(incident_data)\n",
        "  all_incident_results.append(analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9RtDetuO2Wy",
        "outputId": "74bd8716-f131-43cd-c1f1-0ef92cd9f033"
      },
      "id": "Z9RtDetuO2Wy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing incident 1/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"gemini_incident_analysis.json1\", \"w\") as f:\n",
        "  for r in all_incident_results:\n",
        "    f.write(r + \"\\n\")"
      ],
      "metadata": {
        "id": "3iXroaojQYnE"
      },
      "id": "3iXroaojQYnE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re # Import regex for removing markdown fences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Process all_incident_results to parse the JSON strings\n",
        "processed_gemini_outputs = []\n",
        "for raw_gemini_output in all_incident_results:\n",
        "    # Remove markdown code block fences (e.g., ```json\\n...\\n```)\n",
        "    clean_json_str = re.sub(r'```json\\n|```', '', raw_gemini_output).strip()\n",
        "    try:\n",
        "        parsed_data = json.loads(clean_json_str)\n",
        "        # Gemini might return a list of objects or a single object\n",
        "        if isinstance(parsed_data, list):\n",
        "            processed_gemini_outputs.extend(parsed_data)\n",
        "        else:\n",
        "            processed_gemini_outputs.append(parsed_data)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Warning: Failed to decode JSON from Gemini output: {e}. Data: {clean_json_str[:100]}...\")\n",
        "        # If parsing fails, you might want to skip this item or add a placeholder\n",
        "        continue\n",
        "\n",
        "# Extract text for clustering. 'signature' from Gemini's analysis seems appropriate.\n",
        "incident_texts = [item.get(\"signature\", \"\") for item in processed_gemini_outputs if isinstance(item, dict)]\n",
        "\n",
        "# Filter out any empty signatures, as they won't contribute to clustering\n",
        "incident_texts = [text for text in incident_texts if text.strip()]\n",
        "\n",
        "# Ensure there's enough data to cluster\n",
        "if len(incident_texts) == 0:\n",
        "    print(\"No valid incident texts (signatures) found for clustering.\")\n",
        "    # Clear all_incident_results if no valid data to prevent further errors\n",
        "    all_incident_results = []\n",
        "elif len(incident_texts) < 3: # KMeans needs at least n_samples >= n_clusters for default n_clusters=3\n",
        "    print(f\"Warning: Only {len(incident_texts)} incident texts found. Adjusting number of clusters.\")\n",
        "    num_clusters = max(1, len(incident_texts)) # Use at least 1 cluster if data exists\n",
        "    if num_clusters == 1:\n",
        "        labels = [0] * len(incident_texts) # All in one cluster\n",
        "    else:\n",
        "        vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "        X = vectorizer.fit_transform(incident_texts)\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10) # n_init for newer KMeans\n",
        "        labels = kmeans.fit_predict(X)\n",
        "else:\n",
        "    # Vectorize alerts\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(incident_texts)\n",
        "\n",
        "    # Choose num_clusters = 3 for demo\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # n_init for newer KMeans\n",
        "    labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Attach clusters to the processed Gemini outputs\n",
        "for idx, item in enumerate(processed_gemini_outputs):\n",
        "    if idx < len(labels):\n",
        "        item[\"cluster\"] = int(labels[idx])\n",
        "\n",
        "print(\"\\n=== CLUSTERED INCIDENTS ===\\n\")\n",
        "for item in processed_gemini_outputs:\n",
        "    text_to_display = item.get(\"signature\", str(item)) # Display signature or full item if no signature\n",
        "    cluster_info = item.get(\"cluster\", \"N/A\")\n",
        "    print(f\"Cluster {cluster_info} \\u2192 {text_to_display}\")\n",
        "\n",
        "# Update all_incident_results to contain the processed and clustered dictionaries\n",
        "all_incident_results = processed_gemini_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHI7-eFPbrXV",
        "outputId": "5e5d3163-c09f-46c7-ce9f-366a24ca3b53"
      },
      "id": "sHI7-eFPbrXV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CLUSTERED INCIDENTS ===\n",
            "\n",
            "Cluster 0 → Downstream DB Timeouts\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Application High CPU & OOM Error\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Cache Eviction Thrashing / Connection Reset\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 0 → Inter-Region Network Packet Loss\n",
            "Cluster 0 → Application High CPU & Auto-scale Failure\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Downstream DB Timeouts\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & ThreadPool Exhaustion\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 0 → Cache Eviction Thrashing\n",
            "Cluster 0 → Downstream DB Timeouts\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Downstream Cluster Timeouts\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Query Timeout\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Inter-Region Network Packet Loss\n",
            "Cluster 0 → Application High CPU & Connection/Query Timeout\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Downstream Infra Connection Reset / Timeouts\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 0 → Cache Eviction Thrashing\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Auto-scale Failure\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Downstream DB Timeouts\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Query Timeout\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 0 → Inter-Region Network Packet Loss\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Connection/Query Timeout\n",
            "Cluster 0 → Downstream DB Timeouts\n",
            "Cluster 0 → Cache Eviction Thrashing\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & ThreadPool Exhaustion\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 0 → Inter-Region Network Packet Loss\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Query Timeout\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Downstream Cluster Timeouts\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Connection/Query Timeout\n",
            "Cluster 0 → InvalidTokenException Authentication Failures\n",
            "Cluster 0 → Cache Eviction Thrashing\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Auto-scale Failure\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n",
            "Cluster 0 → Inter-Region Network Packet Loss\n",
            "Cluster 0 → Downstream DB Timeouts\n",
            "Cluster 1 → OOMKilled Pod Restart Loop\n",
            "Cluster 0 → Application High CPU & Query Timeout\n",
            "Cluster 2 → High Disk Usage / Failed Cleanup Job\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Auto-Root-Cause Analysis\n",
        "cluster_map = {}\n",
        "for r in all_incident_results:\n",
        "    cluster_map.setdefault(r[\"cluster\"], []).append(r[\"signature\"])\n",
        "\n",
        "rca_results = {}\n",
        "\n",
        "for cluster_id, incidents in cluster_map.items():\n",
        "    prompt = f\"\"\"\n",
        "    These incidents appear to be correlated:\n",
        "    {incidents}\n",
        "\n",
        "    Give:\n",
        "    1) Probable Root Cause\n",
        "    2) Recommended Fix\n",
        "    3) Monitoring Improvement Suggestions\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    rca_results[cluster_id] = response.text\n",
        "\n",
        "print(\"\\n=== ROOT CAUSE ANALYSIS (AI GENERATED) ===\\n\")\n",
        "for cid, text in rca_results.items():\n",
        "    print(f\"Cluster {cid}: \\n{text}\\n{'-'*80}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t86xLIm3dChs",
        "outputId": "8c7c8df3-7e53-452d-fcc1-b6aa9ba50002"
      },
      "id": "t86xLIm3dChs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ROOT CAUSE ANALYSIS (AI GENERATED) ===\n",
            "\n",
            "Cluster 0: \n",
            "Based on the correlated incidents, here's a breakdown:\n",
            "\n",
            "---\n",
            "\n",
            "### 1) Probable Root Cause\n",
            "\n",
            "The confluence of `Inter-Region Network Packet Loss`, various `Downstream Timeouts` (DB, Cluster, Infra), and widespread `Application High CPU` issues (leading to OOM, ThreadPool Exhaustion, and scaling failures) points to a systemic bottleneck.\n",
            "\n",
            "**Primary Root Cause: Widespread Inter-Region Network Instability and/or Resource Exhaustion Leading to Cascade Failures.**\n",
            "\n",
            "**Explanation:**\n",
            "1.  **Inter-Region Network Packet Loss:** This is a critical indicator. If there's significant packet loss between regions, it will directly impact the performance and reliability of any service that communicates across these boundaries. This would immediately explain `Downstream DB Timeouts`, `Downstream Cluster Timeouts`, and `Downstream Infra Connection Reset / Timeouts` if those resources are in a different region or if the application needs to cross regions to reach them.\n",
            "2.  **Application High CPU & Related Errors:** When downstream services are slow or unavailable due to network issues, applications often suffer. They spend more time waiting for responses, retrying failed requests, and managing connection pools. This \"busy waiting\" or excessive retry logic can cause:\n",
            "    *   **High CPU:** Due to constant retries, context switching, or inefficient handling of stalled connections.\n",
            "    *   **ThreadPool Exhaustion:** Threads get tied up waiting for slow downstream responses, leading to new requests being rejected or queued indefinitely.\n",
            "    *   **OOM Errors:** Excessive retries, large error queues, or inefficient handling of large responses/buffers under duress can exhaust memory.\n",
            "    *   **Query/Connection Timeout:** The application itself starts timing out its own calls to the (already struggling) downstream systems.\n",
            "    *   **Auto-scale Failure:** If application instances are unhealthy (high CPU, OOM), auto-scaling might fail to provision new instances, or the new instances might immediately become unhealthy, exacerbating the problem.\n",
            "3.  **InvalidTokenException Authentication Failures:** If the authentication service is an external dependency (possibly in another region or suffering from the same network/load issues), packet loss or its own resource exhaustion would lead to authentication failures.\n",
            "4.  **Cache Eviction Thrashing / Connection Reset:** If the downstream DB is slow, the application might lean harder on the cache, causing it to thrash. Alternatively, if the cache itself is distributed or relies on inter-region communication, network packet loss could cause cache connection resets and contribute to thrashing as parts of the cache become unreachable or inconsistent. The application would then hit the (already slow) database more often, creating a vicious cycle.\n",
            "\n",
            "In essence, network instability creates a ripple effect: slow downstream calls -> application resource exhaustion (CPU, memory, threads) -> internal application timeouts/failures -> cascading failures across services.\n",
            "\n",
            "---\n",
            "\n",
            "### 2) Recommended Fix\n",
            "\n",
            "The fixes should address the underlying network instability, improve application resilience, and optimize downstream dependencies.\n",
            "\n",
            "1.  **Immediate Action: Investigate and Resolve Inter-Region Network Packet Loss:**\n",
            "    *   **Engage Network Operations:** This is the most critical first step. Identify the specific inter-region links experiencing packet loss.\n",
            "    *   **Network Diagnostics:** Perform extensive diagnostics (traceroute, MTR, ping tests with increased packet size) from application hosts to downstream dependencies (DB, Cache, Auth service) across regions.\n",
            "    *   **Routing & Peering Review:** Examine inter-region routing tables and peering agreements.\n",
            "    *   **Redundancy & Failover:** Verify network path redundancy and failover mechanisms are functioning correctly. Consider using dedicated interconnects or VPNs with better SLAs if relying solely on public internet.\n",
            "\n",
            "2.  **Enhance Application Resilience and Resource Management:**\n",
            "    *   **Implement Circuit Breakers:** For all calls to downstream services (DB, Cache, Auth, etc.). This prevents a failing dependency from taking down the entire application by quickly failing requests instead of waiting for timeouts.\n",
            "    *   **Retry Mechanisms with Jitter & Backoff:** Ensure all retry logic for downstream calls includes exponential backoff and jitter to avoid overwhelming services, and define sensible maximum retry attempts.\n",
            "    *   **Asynchronous Processing:** For long-running or potentially slow operations, use asynchronous programming models to avoid tying up threads from the thread pool.\n",
            "    *   **Tune Thread Pools:** Optimize thread pool sizes for downstream connections (DB, HTTP clients) to match the expected load and latency, preventing exhaustion.\n",
            "    *   **Memory Profiling & Optimization:** Identify and fix memory leaks or inefficient memory usage that contribute to OOM errors.\n",
            "    *   **Optimize Downstream Queries/Calls:** Profile and optimize SQL queries, API calls, and data serialization/deserialization to reduce latency and resource consumption on both sides.\n",
            "    *   **Review Auto-scaling Policies & Health Checks:** Ensure health checks are robust enough to distinguish between temporary slowdowns and actual service unhealthiness. Adjust scaling policies to be more aggressive if needed, or consider proactive scaling based on leading indicators.\n",
            "\n",
            "3.  **Optimize Downstream Dependencies (DB, Cache, Auth Service):**\n",
            "    *   **Database/Cluster Optimization:** Review query performance, add/optimize indexes, analyze connection usage, scale DB resources (CPU, memory, IOPS), and consider read replicas for offloading read traffic.\n",
            "    *   **Cache Optimization:**\n",
            "        *   **Capacity Increase:** If thrashing due to size, increase cache memory.\n",
            "        *   **Eviction Policy Review:** Adjust eviction policies to retain critical data.\n",
            "        *   **Connection Stability:** Ensure cache connections are resilient to network fluctuations.\n",
            "        *   **Local Caching:** For frequently accessed, less volatile data, explore in-memory caches within the application to reduce network hops.\n",
            "    *   **Authentication Service Resilience:** Ensure the authentication service itself is highly available, scaled appropriately, and resilient to network issues. Consider caching valid tokens locally within the application for a short duration to reduce authentication service calls.\n",
            "\n",
            "---\n",
            "\n",
            "### 3) Monitoring Improvement Suggestions\n",
            "\n",
            "To proactively detect these issues and diagnose them faster, focus on end-to-end visibility.\n",
            "\n",
            "1.  **Network-Specific Monitoring:**\n",
            "    *   **Inter-Region Latency & Packet Loss:** Monitor `ping` and `traceroute` statistics *between* critical application hosts and all downstream dependencies (DB, Cache, Auth) in different regions. Alert on spikes in latency or packet loss.\n",
            "    *   **Network Device Health:** Monitor CPU, memory, and interface error rates on all relevant routers, switches, and firewalls involved in inter-region traffic.\n",
            "    *   **Bandwidth Utilization:** Monitor network interface utilization on application servers and network devices to detect congestion.\n",
            "\n",
            "2.  **Application Performance Monitoring (APM):**\n",
            "    *   **Request Latency & Error Rates:** Granular monitoring of end-to-end request latency, broken down by stages (e.g., application processing, DB time, external service calls). Track overall error rates and specific error types (timeouts, auth failures).\n",
            "    *   **Resource Utilization:** Detailed CPU, Memory, Disk I/O, and Network I/O metrics per instance.\n",
            "    *   **JVM/Runtime Metrics:** (If applicable) Garbage Collection duration and frequency, Thread Pool usage (active vs. waiting threads, queue length), connection pool statistics (active, idle, waiting connections).\n",
            "    *   **Downstream Dependency Latency:** Instrument calls to all external services (DB queries, cache operations, authentication API calls) to track their individual latencies and error rates.\n",
            "    *   **Circuit Breaker State:** Monitor the state of circuit breakers (open, closed, half-open) to understand when dependencies are failing.\n",
            "    *   **Retry Counts:** Track the number of retries being performed by the application to downstream services. High retry counts are a good indicator of underlying issues.\n",
            "\n",
            "3.  **Downstream Dependency Monitoring:**\n",
            "    *   **Database/Cluster:** Query execution times (slow query logs), connection count, active sessions, CPU/memory usage, disk I/O, lock contention.\n",
            "    *   **Cache:** Cache hit/miss ratio, eviction rates, memory usage, connection count.\n",
            "    *   **Authentication Service:** API response times, error rates, resource utilization.\n",
            "\n",
            "4.  **Auto-scaling & Infrastructure Monitoring:**\n",
            "    *   **Auto-scaling Events:** Monitor successful and failed scaling events, and the health status of instances in the scaling group.\n",
            "    *   **Load Balancer Metrics:** Request counts, backend health checks, latency, 5xx rates.\n",
            "\n",
            "5.  **Alerting Improvements:**\n",
            "    *   **Threshold-based Alerts:** Set up alerts for deviations from normal baselines for all the metrics listed above.\n",
            "    *   **Composite Alerts:** Create alerts that trigger only when multiple correlated symptoms appear (e.g., \"High App CPU *AND* High DB Latency *AND* Inter-Region Packet Loss\") to reduce noise and pinpoint root causes faster.\n",
            "    *   **Early Warning Indicators:** Use metrics like retry counts, increased queue depths, or rising connection pool waits as early warning signs before full-blown timeouts or OOM errors occur.\n",
            "--------------------------------------------------------------------------------\n",
            "Cluster 2: \n",
            "This incident pattern clearly indicates a persistent problem where automated disk cleanup is failing, leading to consistently high disk usage.\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "---\n",
            "\n",
            "### 1) Probable Root Cause\n",
            "\n",
            "The most probable root cause lies in the **failure or inadequacy of the cleanup job itself**, exacerbated by ongoing data generation.\n",
            "\n",
            "*   **Cleanup Job Failure/Misconfiguration:**\n",
            "    *   **Permissions Issues:** The user/service account running the cleanup job might lack the necessary read/write/delete permissions for the directories it's supposed to clean.\n",
            "    *   **File Locks/In-Use Files:** The cleanup job might be attempting to delete files that are actively in use or locked by another process, causing it to fail or skip them.\n",
            "    *   **Script/Configuration Errors:** The cleanup script or its configuration might have a bug, incorrect paths, faulty logic, or an incorrect filter (e.g., not deleting the right types of files, or not going deep enough into directories).\n",
            "    *   **Resource Exhaustion (Cleanup Job Itself):** While less common for cleanup, the job itself might fail due to insufficient memory or CPU if it's processing an extremely large number of files.\n",
            "    *   **Scheduler Issues:** The job might not be running at all, or not completing within its allotted time due to system load or misconfiguration in the scheduler (e.g., Cron, Task Scheduler).\n",
            "    *   **Dependent Service Failure:** The cleanup job might depend on a database, network share, or API that is currently unavailable, causing it to fail.\n",
            "\n",
            "*   **Excessive Data Generation:**\n",
            "    *   **Uncontrolled Log Growth:** Application or system logs are being generated at an unusually high rate (e.g., due to increased logging levels, debugging accidentally left on, or an application error loop).\n",
            "    *   **Temporary File Accumulation:** Applications are creating temporary files but failing to delete them after use.\n",
            "    *   **Backup/Snapshot Accumulation:** Old backups, snapshots, or intermediate processing files are not being purged according to policy.\n",
            "    *   **Application Bug:** A recent deployment or specific feature might be generating an unexpected volume of data.\n",
            "\n",
            "---\n",
            "\n",
            "### 2) Recommended Fix\n",
            "\n",
            "The fix involves a multi-pronged approach to address both the cleanup job and the data generation.\n",
            "\n",
            "1.  **Immediate Remediation (to clear space):**\n",
            "    *   **Manually Identify & Delete:** Log into the affected system(s), identify the largest directories/files using `du -sh *` (Linux/macOS) or Disk Cleanup/TreeSize Free (Windows), and manually delete non-critical, old, or excessive files (e.g., old logs, temporary files, old backups). **Exercise extreme caution to avoid deleting critical system or application files.**\n",
            "    *   **Restart Applications:** Sometimes files remain locked by a process; restarting the application (if safe to do so) might release locks allowing manual deletion.\n",
            "\n",
            "2.  **Troubleshoot and Repair Cleanup Job:**\n",
            "    *   **Review Cleanup Job Logs:** The *first and most critical step* is to examine the output/logs of the failed cleanup job. It should contain error messages detailing *why* it failed (e.g., permission denied, file in use, script error).\n",
            "    *   **Verify Permissions:** Ensure the user/service account running the cleanup job has full read/write/delete permissions on all target directories.\n",
            "    *   **Inspect Cleanup Script/Configuration:**\n",
            "        *   Are the target paths correct?\n",
            "        *   Are the deletion criteria (age, file type, size) appropriate?\n",
            "        *   Is there any logic that could cause it to skip files or fail silently?\n",
            "        *   Test the script manually with verbose logging.\n",
            "    *   **Address File Locks:** If logs indicate files are locked, identify the locking process. This might require adjusting the cleanup schedule (e.g., run when the application is less active), configuring the application to release files, or adding logic to the cleanup script to handle locked files gracefully (e.g., retry, skip and log).\n",
            "    *   **Increase Job Frequency/Aggressiveness:** If the job is running but not keeping up, consider:\n",
            "        *   Running it more frequently.\n",
            "        *   Adjusting retention policies to delete older files sooner.\n",
            "        *   Expanding the scope of files/directories it targets.\n",
            "\n",
            "3.  **Address Excessive Data Generation:**\n",
            "    *   **Identify Data Source:** Determine *which application* or system process is generating the excessive data.\n",
            "    *   **Optimize Logging:**\n",
            "        *   Review and reduce log levels (e.g., from DEBUG to INFO) in production environments.\n",
            "        *   Implement log rotation and compression policies more aggressively.\n",
            "        *   Consider centralized log management with retention policies.\n",
            "    *   **Application-Specific Cleanup:** Implement internal cleanup mechanisms within the application itself for temporary files, caches, or old data that might not be covered by generic cleanup jobs.\n",
            "    *   **Storage Expansion/Migration (Last Resort):** If the data generation is legitimate and cannot be reduced, consider increasing the disk size or migrating high-volume data to more appropriate long-term storage solutions (e.g., object storage, cheaper archiving). This should be a last resort after optimization efforts.\n",
            "\n",
            "---\n",
            "\n",
            "### 3) Monitoring Improvement Suggestions\n",
            "\n",
            "To prevent recurrence and provide earlier warning:\n",
            "\n",
            "1.  **Direct Cleanup Job Monitoring:**\n",
            "    *   **Job Status Alerts:** Configure alerts for any non-zero exit code or explicit \"FAILURE\" status from the cleanup job's execution.\n",
            "    *   **Job Duration Alerts:** Alert if the cleanup job runs significantly longer or shorter than expected, indicating it might be stuck or failing quickly.\n",
            "    *   **Job Output Analysis:** Monitor the job's log output for specific keywords like \"Permission denied,\" \"Access denied,\" \"Failed to delete,\" or \"Error,\" and trigger alerts.\n",
            "    *   **Bytes Freed Metric:** Instrument the cleanup job to report the amount of disk space it successfully freed. Alert if this metric is zero or unusually low when the job runs.\n",
            "\n",
            "2.  **Enhanced Disk Usage Monitoring:**\n",
            "    *   **Multi-Tier Thresholds:** Implement multiple disk usage thresholds (e.g., Warning at 70%, Critical at 85%, Severe at 90%).\n",
            "    *   **Rate of Change Monitoring:** Monitor the *rate of increase* in disk usage. An alert should trigger if disk usage grows by more than X% in Y hours/days, even if the absolute percentage is still below critical. This can catch runaway processes early.\n",
            "    *   **Per-Directory/Mount Point Monitoring:** Instead of just overall disk usage, monitor the usage of critical directories or mount points where high disk usage typically occurs (e.g., `/var/log`, `/tmp`, application data directories). This helps pinpoint the source.\n",
            "\n",
            "3.  **Application/Log Generation Monitoring:**\n",
            "    *   **Log File Size Monitoring:** Monitor the size of key log files or log directories. Alert if they grow rapidly or exceed expected thresholds within a short period.\n",
            "    *   **Application Metrics:** If possible, implement application-level metrics for temporary file creation, data generation, or cache size, and alert on anomalies.\n",
            "    *   **I/O Operations:** Monitor disk I/O metrics to identify processes writing an unusually high volume of data to disk.\n",
            "\n",
            "4.  **Dashboards and Reporting:**\n",
            "    *   Create a dashboard showing disk usage trends over time, overlaid with cleanup job execution times and success/failure status. This visual correlation can help identify patterns.\n",
            "    *   Regular reports on disk space utilization across systems.\n",
            "\n",
            "By combining these proactive monitoring and resolution strategies, you can effectively manage disk space and prevent future incidents of this type.\n",
            "--------------------------------------------------------------------------------\n",
            "Cluster 1: \n",
            "An \"OOMKilled Pod Restart Loop\" indicates that a Kubernetes pod is being terminated by the operating system (or the kubelet) because it has exceeded its allocated memory limit, and Kubernetes is then automatically restarting it, only for the cycle to repeat. This is a critical issue indicating either a misconfigured resource limit or a problem within the application itself.\n",
            "\n",
            "---\n",
            "\n",
            "### 1) Probable Root Cause\n",
            "\n",
            "The core issue is that the application within the pod is attempting to use more memory than it is allowed. Here are the most probable causes:\n",
            "\n",
            "1.  **Insufficient Memory Limits:**\n",
            "    *   **Description:** The `memory.limits` defined in the pod's Kubernetes manifest (or its container definition) are set too low for the application's actual memory requirements. This is the most common cause.\n",
            "    *   **Why it happens:** Initial estimates were too low, application changed, or load increased without adjusting limits.\n",
            "2.  **Memory Leak in the Application:**\n",
            "    *   **Description:** The application itself has a bug where it continuously allocates memory without properly releasing it. Over time, its memory footprint grows until it hits the limit.\n",
            "    *   **Why it happens:** Unmanaged object references, unclosed resources (file handles, network connections, database connections), unbounded caches, or faulty garbage collection logic.\n",
            "3.  **Increased Workload/Load Spikes:**\n",
            "    *   **Description:** While the application might behave normally under average load, a sudden surge in traffic or a particularly complex request might cause it to temporarily require significantly more memory, pushing it over the limit.\n",
            "    *   **Why it happens:** Unexpected user behavior, marketing campaigns, data processing peaks.\n",
            "4.  **Inefficient Application Code/Configuration:**\n",
            "    *   **Description:** The application's algorithms, data structures, or internal configurations (e.g., thread pools, buffer sizes) are inherently memory-intensive, especially when processing certain types of data or under specific conditions.\n",
            "    *   **Why it happens:** Poorly optimized code, default configurations that don't suit the workload, or processing very large payloads.\n",
            "5.  **Sidecar Container Memory Usage:**\n",
            "    *   **Description:** If the pod contains multiple containers, and they share the same memory cgroup (which they do by default), one container could be consuming an excessive amount of memory, starving others or collectively exceeding the pod's limit.\n",
            "\n",
            "---\n",
            "\n",
            "### 2) Recommended Fix\n",
            "\n",
            "The fix depends on the identified root cause. It's often an iterative process.\n",
            "\n",
            "1.  **Immediate Action (Band-Aid for critical systems):**\n",
            "    *   **Increase `memory.limits` (and `memory.requests`):** As a temporary measure to get the service running again, cautiously increase the `memory.limits` (and ideally `memory.requests` to match or be slightly below limits to ensure scheduling) for the affected pod.\n",
            "    *   **Caution:** This is *not* a permanent solution if there's a memory leak. It merely postpones the inevitable and can hide the underlying issue, potentially leading to node instability.\n",
            "\n",
            "2.  **Long-Term Solutions (Root Cause Resolution):**\n",
            "\n",
            "    *   **If Insufficient Memory Limits:**\n",
            "        *   **Profiling and Performance Testing:** Use profiling tools (e.g., `top`, `kubectl top pod`, application-specific profilers, APM tools) during representative workloads to determine the *actual* peak memory usage of the application.\n",
            "        *   **Adjust Resource Limits:** Set `memory.requests` to the average working set and `memory.limits` to the observed peak usage plus a comfortable buffer (e.g., 10-20%).\n",
            "        *   **Horizontal Scaling:** Consider using a Horizontal Pod Autoscaler (HPA) based on memory usage if increased load is the primary driver, allowing more pods to handle the load collectively.\n",
            "\n",
            "    *   **If Memory Leak in Application:**\n",
            "        *   **Application Profiling:** Use language-specific profiling tools (e.g., Java Heap Dump Analysis, Python `memory_profiler`, Go `pprof`, Node.js `heapdump`) to identify where memory is being allocated but not released.\n",
            "        *   **Code Review and Debugging:** Carefully review the application code for common memory leak patterns (e.g., unclosed resources, static collections, circular references, improper event listener cleanup).\n",
            "        *   **Deploy Fix:** Patch the application and deploy the new version.\n",
            "\n",
            "    *   **If Increased Workload/Load Spikes:**\n",
            "        *   **Optimize Application:** Improve code efficiency to reduce memory footprint per request.\n",
            "        *   **Implement HPA:** Set up HPA to scale pods based on memory utilization (or CPU, if correlated) to handle load spikes gracefully.\n",
            "        *   **Rate Limiting:** Implement rate limiting at the ingress or application level to prevent overwhelming the service.\n",
            "\n",
            "    *   **If Inefficient Application Code/Configuration:**\n",
            "        *   **Code Optimization:** Refactor algorithms, use more memory-efficient data structures, or re-evaluate third-party library usage.\n",
            "        *   **Configuration Tuning:** Adjust application-specific memory settings (e.g., JVM heap size, garbage collection parameters, buffer sizes) to better suit the environment and workload.\n",
            "\n",
            "    *   **If Sidecar Container Issues:**\n",
            "        *   **Isolate Resource Limits:** If possible and if the sidecar is independent, consider moving it to its own pod or, if within the same pod, ensure its resource usage is tracked and factored into the total pod limits.\n",
            "        *   **Review Sidecar Configuration/Behavior:** Troubleshoot the sidecar itself for memory leaks or excessive usage.\n",
            "\n",
            "---\n",
            "\n",
            "### 3) Monitoring Improvement Suggestions\n",
            "\n",
            "To prevent future OOMKilled incidents and diagnose them more quickly, enhance your monitoring:\n",
            "\n",
            "1.  **Granular Memory Metrics:**\n",
            "    *   **Pod Memory Usage:** Collect and visualize actual memory usage (`container_memory_usage_bytes`, `container_memory_working_set_bytes`) for each container within the pod, alongside its `memory.limits` and `memory.requests`.\n",
            "    *   **Node Memory Usage:** Monitor overall node memory usage (`node_memory_MemAvailable_bytes`) to ensure the nodes themselves aren't under pressure, which could exacerbate pod-level issues.\n",
            "    *   **Application-Specific Metrics:** For JVM applications, expose and monitor specific JVM memory pools (heap, non-heap, garbage collection activity). Similar metrics for other runtimes (e.g., Go `memstats`, Node.js `process.memoryUsage()`).\n",
            "\n",
            "2.  **Alerting on Memory Thresholds:**\n",
            "    *   **Near Memory Limit:** Alert when a pod's memory usage consistently exceeds a high percentage of its limit (e.g., 80-90%) for a sustained period. This is a pre-emptive warning.\n",
            "    *   **OOMKilled Events:** Set up alerts for `OOMKilled` events in Kubernetes (e.g., `kube_pod_container_status_restarts_total` increasing rapidly for a specific container, or monitoring `kubectl describe pod` for OOMKilled messages).\n",
            "    *   **High Restart Rate:** Alert if a pod's restart count increases rapidly within a short timeframe, indicating a restart loop.\n",
            "\n",
            "3.  **Kubernetes Event Monitoring:**\n",
            "    *   Utilize a Kubernetes event exporter (e.g., `kube-state-metrics` combined with Prometheus/Grafana) to capture and visualize `OOMKilled` events and pod lifecycle events.\n",
            "\n",
            "4.  **Historical Data and Trends:**\n",
            "    *   Maintain long-term storage of memory metrics to analyze trends over time. This helps identify gradual memory leaks or seasonal load increases.\n",
            "    *   Compare memory usage patterns before and after deployments to detect regressions.\n",
            "\n",
            "5.  **Application Performance Monitoring (APM) Tools:**\n",
            "    *   Integrate APM solutions (e.g., Datadog, New Relic, Dynatrace, Prometheus/Grafana with application instrumentation) to gain deep insights into application memory allocation, object lifetimes, and garbage collection behavior. This is crucial for pinpointing memory leaks.\n",
            "\n",
            "6.  **Resource Request/Limit Review Process:**\n",
            "    *   Periodically review and validate the resource requests and limits for critical applications based on actual performance data and load tests. This should be part of your release process for major updates.\n",
            "\n",
            "By implementing these suggestions, you'll not only resolve the current incident but also build a more resilient and observable system that can proactively identify and mitigate memory-related issues.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SRE Traits - Add severity prediction\n",
        "\n",
        "for r in all_incident_results:\n",
        "    prompt = f\"\"\"\n",
        "    Incident: {r['signature']}\n",
        "    Predict severity (P1/P2/P3) and justify.\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "    r[\"severity\"] = response.text.strip()"
      ],
      "metadata": {
        "id": "YargKtNZeuB9"
      },
      "id": "YargKtNZeuB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output = {\n",
        "    \"incidents\": all_incident_results,\n",
        "    \"rca\": rca_results\n",
        "}\n",
        "\n",
        "with open(\"final_aiops_report.json\", \"w\") as f:\n",
        "    json.dump(output, f, indent=4)"
      ],
      "metadata": {
        "id": "slinGV9th6_z"
      },
      "id": "slinGV9th6_z",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}